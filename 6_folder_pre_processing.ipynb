{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709fd69-ad83-4da4-b9dc-4a779091d9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b7f3407-c604-4e68-8022-697b9f968e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Script to Add RX Sphere Features ---\n",
      "\n",
      "▶ Processing 7GHz | Tx 5\n",
      "⚠️ Skipping missing CSV file: /home/mkrishne/PL_competition/catboost_training/for_catboost_test/7GHz_Tx_5_test_data.csv\n",
      "\n",
      "▶ Processing 7GHz | Tx 9\n",
      "⚠️ Skipping missing CSV file: /home/mkrishne/PL_competition/catboost_training/for_catboost_test/7GHz_Tx_9_test_data.csv\n",
      "\n",
      "▶ Processing 7GHz | Tx 12\n",
      "⚠️ Skipping missing CSV file: /home/mkrishne/PL_competition/catboost_training/for_catboost_test/7GHz_Tx_12_test_data.csv\n",
      "\n",
      "▶ Processing 7GHz | Tx 14\n",
      "⚠️ Skipping missing CSV file: /home/mkrishne/PL_competition/catboost_training/for_catboost_test/7GHz_Tx_14_test_data.csv\n",
      "\n",
      "▶ Processing 7GHz | Tx 20\n",
      "⚠️ Skipping missing CSV file: /home/mkrishne/PL_competition/catboost_training/for_catboost_test/7GHz_Tx_20_test_data.csv\n",
      "\n",
      "\n",
      "All files processed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle  # For catching UnpicklingError\n",
    "from zipfile import BadZipFile \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# --- Overrides for testing (optional) ---\n",
    "freqs = [\"7GHz\"]\n",
    "txids = [1]\n",
    "\n",
    "# Directories\n",
    "base_dir = r\"C:\\Users\\mkrishne\\OneDrive - purdue.edu\\ECNDATA\\Desktop\\PL_competition\\c_evaluation_propagation_loss\"\n",
    "save_dir = r\"C:\\Users\\mkrishne\\OneDrive - purdue.edu\\ECNDATA\\Desktop\\PL_competition\\catboost_training\\for_catboost_train\"\n",
    "region_dir = r\"C:\\Users\\mkrishne\\OneDrive - purdue.edu\\ECNDATA\\Desktop\\PL_competition\\extracted_regions\\train\"\n",
    "\n",
    "# --- Master CSV directory is constant ---\n",
    "MASTER_CSV_DIR = r\"C:\\Users\\mkrishne\\OneDrive - purdue.edu\\ECNDATA\\Desktop\\PL_competition\\c_evaluation_propagation_loss\\800MHz\"\n",
    "\n",
    "# Coordinate shift\n",
    "xmin = 384592.1875\n",
    "ymin = 3944795.0\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def calculate_polygon_area(polygon):\n",
    "    \"\"\"Calculates the total area of a single 3D polygon.\"\"\"\n",
    "    if len(polygon) < 3:\n",
    "        return 0.0\n",
    "    total_area = 0.0\n",
    "    v0 = polygon[0]\n",
    "    for i in range(1, len(polygon) - 1):\n",
    "        v1 = polygon[i]\n",
    "        v2 = polygon[i+1]\n",
    "        area = 0.5 * np.linalg.norm(np.cross(v1 - v0, v2 - v0))\n",
    "        total_area += area\n",
    "    return total_area\n",
    "\n",
    "def get_sphere_features(npz_file_path):\n",
    "    \"\"\"\n",
    "    Calculates polygon count, total area, and average max height\n",
    "    from a single ..._sphere50.npz file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = np.load(npz_file_path, allow_pickle=True)\n",
    "        polygons = data['polys'].tolist()\n",
    "    except FileNotFoundError:\n",
    "        return \"MISSING\", 0.0, 0.0\n",
    "    # Catches all known file corruption errors\n",
    "    except (BadZipFile, EOFError, pickle.UnpicklingError, KeyError) as e:\n",
    "        # KeyError handles missing 'polys' key\n",
    "        return \"CORRUPT\", 0.0, 0.0\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected error during loading\n",
    "        return f\"ERROR: {e}\", 0.0, 0.0\n",
    "    \n",
    "    if not polygons:\n",
    "        return 0, 0.0, 0.0 \n",
    "\n",
    "    try:\n",
    "        poly_count = len(polygons)\n",
    "        obstr_poly_area = 0.0\n",
    "        max_heights_per_polygon = []\n",
    "        \n",
    "        for poly in polygons:\n",
    "            obstr_poly_area += calculate_polygon_area(poly)\n",
    "            # Add check for empty polygon data\n",
    "            if len(poly) > 0:\n",
    "                max_heights_per_polygon.append(np.max(poly[:, 2]))\n",
    "        \n",
    "        avg_clutter_height = np.mean(max_heights_per_polygon) if max_heights_per_polygon else 0.0\n",
    "        \n",
    "        return poly_count, obstr_poly_area, avg_clutter_height\n",
    "    except Exception as e:\n",
    "        # Catch errors during calculation (e.g., np.max on empty array)\n",
    "        return f\"ERROR: {e}\", 0.0, 0.0\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Main Loop (Corrected Logic)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"--- Starting Script to Add RX Sphere Features ---\")\n",
    "for freq in freqs:\n",
    "    for tx_id in txids:\n",
    "        print(f\"\\n▶ Processing {freq} | Tx {tx_id}\")\n",
    "\n",
    "        # 1. Define and load the final CSV file to be augmented\n",
    "        input_file = os.path.join(save_dir, f\"{freq}_Tx_{tx_id}_train_data.csv\")\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"⚠️ Skipping missing CSV file: {input_file}\")\n",
    "            continue\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"  -> Loaded CSV with {len(df)} rows.\")\n",
    "        \n",
    "        if \"avg_rx_clutter_height\" in df.columns:\n",
    "            print(f\"  -> Columns already exist. Skipping this file.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Load the RX coordinates from the *base* CSV\n",
    "        orig_csv_path = os.path.join(base_dir, freq, f\"{freq}_Tx_{tx_id}.csv\")\n",
    "        if not os.path.exists(orig_csv_path):\n",
    "            print(f\"⚠️ Missing base file for RX coords: {orig_csv_path}. Skipping.\")\n",
    "            continue\n",
    "        df_orig = pd.read_csv(orig_csv_path)\n",
    "        \n",
    "        # Ensure row count matches\n",
    "        if len(df_orig) != len(df):\n",
    "             print(f\"  -> ❌ ERROR: Mismatch in row count between {input_file} ({len(df)}) and {orig_csv_path} ({len(df_orig)}). Skipping.\")\n",
    "             continue\n",
    "        rx_all = df_orig.iloc[:, [6, 7, 8]].values\n",
    "        rx_all_shifted = rx_all.copy()\n",
    "        rx_all_shifted[:, 0] -= xmin\n",
    "        rx_all_shifted[:, 1] -= ymin\n",
    "\n",
    "        # 3. Load the RX coordinates from the *master* CSV (for lookup)\n",
    "        master_csv_path = os.path.join(MASTER_CSV_DIR, f\"800MHz_Tx_{tx_id}.csv\")\n",
    "        if not os.path.exists(master_csv_path):\n",
    "            print(f\"⚠️ Missing master file for lookup: {master_csv_path}. Skipping.\")\n",
    "            continue\n",
    "        df_master = pd.read_csv(master_csv_path)\n",
    "        rx_all_master = df_master.iloc[:, [6, 7, 8]].values\n",
    "        rx_all_master_shifted = rx_all_master.copy()\n",
    "        rx_all_master_shifted[:, 0] -= xmin\n",
    "        rx_all_master_shifted[:, 1] -= ymin\n",
    "\n",
    "        # 4. Build the fast lookup map: { (x,y,z) -> pair_id }\n",
    "        print(\"  -> Building coordinate lookup map from 800MHz master file...\")\n",
    "        master_lookup = {\n",
    "            tuple(np.round(coord, 5)): i \n",
    "            for i, coord in enumerate(rx_all_master_shifted)\n",
    "        }\n",
    "        print(f\"  -> Map built with {len(master_lookup)} unique locations.\")\n",
    "\n",
    "        # 5. Loop through training RX points, find match, and get features\n",
    "        all_rx_poly_counts = []\n",
    "        all_rx_obstr_areas = []\n",
    "        all_rx_clutter_heights = []\n",
    "        \n",
    "        # Detailed error logging\n",
    "        lookup_failures = []\n",
    "        corrupt_files = []\n",
    "        missing_files = []\n",
    "        other_errors = {}\n",
    "        \n",
    "        npz_dir = os.path.join(region_dir, f\"rx_sphere_Tx_{tx_id}\")\n",
    "        \n",
    "        print(\"  -> Matching RX points and processing NPZ files...\")\n",
    "        # Use df.index to match the rows being processed\n",
    "        for i in tqdm(df.index, desc=\"  Processing RX spheres\"):\n",
    "            rx_pt = rx_all_shifted[i] # Get the corresponding rx coordinate\n",
    "            rx_sphere_pair_id = master_lookup.get(tuple(np.round(rx_pt, 5)))\n",
    "            \n",
    "            if rx_sphere_pair_id is None:\n",
    "                lookup_failures.append(i) # Log the row index\n",
    "                all_rx_poly_counts.append(0)\n",
    "                all_rx_obstr_areas.append(0.0)\n",
    "                all_rx_clutter_heights.append(0.0)\n",
    "                continue\n",
    "\n",
    "            npz_file = os.path.join(npz_dir, f\"pair{rx_sphere_pair_id:05d}_rx_sphere50.npz\")\n",
    "            poly_count, total_area, avg_height = get_sphere_features(npz_file)\n",
    "\n",
    "            if isinstance(poly_count, str): # Check if an error string was returned\n",
    "                if poly_count == \"CORRUPT\":\n",
    "                    corrupt_files.append(rx_sphere_pair_id)\n",
    "                elif poly_count == \"MISSING\":\n",
    "                    missing_files.append(rx_sphere_pair_id)\n",
    "                else: # Catches \"ERROR: ...\"\n",
    "                    other_errors[rx_sphere_pair_id] = poly_count\n",
    "                \n",
    "                # Append defaults\n",
    "                all_rx_poly_counts.append(0)\n",
    "                all_rx_obstr_areas.append(0.0)\n",
    "                all_rx_clutter_heights.append(0.0)\n",
    "            else:\n",
    "                # Success\n",
    "                all_rx_poly_counts.append(poly_count)\n",
    "                all_rx_obstr_areas.append(total_area)\n",
    "                all_rx_clutter_heights.append(avg_height)\n",
    "        \n",
    "        # 6. Insert new columns into the main DataFrame\n",
    "        \n",
    "        # Detailed error reporting\n",
    "        if lookup_failures:\n",
    "            print(f\"  -> ⚠️ WARNING: {len(lookup_failures)} RX points not found in master list. (First 10 indices: {lookup_failures[:10]})\")\n",
    "        if corrupt_files:\n",
    "            print(f\"  -> ⚠️ WARNING: {len(corrupt_files)} corrupt .npz files detected. (First 10 pair_ids: {corrupt_files[:10]})\")\n",
    "        if missing_files:\n",
    "            print(f\"  -> ⚠️ WARNING: {len(missing_files)} missing .npz files detected. (First 10 pair_ids: {missing_files[:10]})\")\n",
    "        if other_errors:\n",
    "            print(f\"  -> ❌ ERROR: {len(other_errors)} other processing errors. (First 5):\")\n",
    "            for k, v in list(other_errors.items())[:5]:\n",
    "                print(f\"     Pair {k}: {v}\")\n",
    "                \n",
    "        try:\n",
    "            insert_loc = df.columns.get_loc(\"measured_pathloss_dB\")\n",
    "            \n",
    "            df.insert(insert_loc, \"rx_sphere_poly_count\", all_rx_poly_counts)\n",
    "            df.insert(insert_loc + 1, \"rx_sphere_obstr_poly_area\", all_rx_obstr_areas)\n",
    "            df.insert(insert_loc + 2, \"avg_rx_clutter_height\", all_rx_clutter_heights)\n",
    "            \n",
    "            # 7. Save the augmented file\n",
    "            df.to_csv(input_file, index=False)\n",
    "            print(f\"  -> ✅ Successfully added features and saved file: {input_file}\")\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"  -> ❌ ERROR: Column 'measured_pathloss_dB' not found. Skipping save.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ❌ ERROR: An unexpected error occurred: {e}. Skipping save.\")\n",
    "\n",
    "print(\"\\n\\nAll files processed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
