{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9af4a52-f935-4a5e-b7b5-627a5d6ba949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.linear_model import LinearRegression,HuberRegressor,RANSACRegressor,TheilSenRegressor,SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,HistGradientBoostingRegressor,AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import neighbors\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "858ec9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "set_seed(42)\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02bf796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs = [\"7GHz\", \"28GHz\", \"800MHz\"]\n",
    "# tx_ids = [1, 2, 8, 18, 19]\n",
    "\n",
    "# for freq in freqs:\n",
    "#     for txid in tx_ids:\n",
    "#         meta_file = f\"./{freq}_Tx_{txid}.csv\"\n",
    "#         train_file = f\"./{freq}_Tx_{txid}_train_data.csv\"\n",
    "#         output_file = f\"./{freq}_Tx_{txid}_train_data_with_coords.csv\"\n",
    "\n",
    "#         if not os.path.exists(meta_file) or not os.path.exists(train_file):\n",
    "#             print(f\"Skipping {freq}_Tx_{txid} ‚Äî missing file(s).\")\n",
    "#             continue\n",
    "\n",
    "#         # --- Read metadata (Tx and Rx coordinates) ---\n",
    "#         meta_df = pd.read_csv(meta_file)\n",
    "#         tx_lon = meta_df.iloc[:, 3].values  # 4th column (Tx_Lon)\n",
    "#         tx_lat = meta_df.iloc[:, 4].values  # 5th column (Tx_Lat)\n",
    "#         rx_lon = meta_df[\"Rx_lon\"].values\n",
    "#         rx_lat = meta_df[\"Rx_lat\"].values\n",
    "\n",
    "#         # --- Read train data ---\n",
    "#         train_df = pd.read_csv(train_file)\n",
    "\n",
    "#         # --- Prepare coordinate DataFrame ---\n",
    "#         coords_df = pd.DataFrame({\n",
    "#             \"Tx_Lon\": tx_lon,\n",
    "#             \"Tx_Lat\": tx_lat,\n",
    "#             \"Rx_Lon\": rx_lon,\n",
    "#             \"Rx_Lat\": rx_lat\n",
    "#         })\n",
    "\n",
    "#         # --- Insert before the last column ---\n",
    "#         insert_pos = len(train_df.columns) - 1\n",
    "#         for col_name in [\"Tx_Lon\", \"Tx_Lat\", \"Rx_Lon\", \"Rx_Lat\"]:\n",
    "#             train_df.insert(insert_pos, col_name, coords_df[col_name])\n",
    "#             insert_pos += 1  # shift forward for next column\n",
    "\n",
    "#         # --- Save output ---\n",
    "#         train_df.to_csv(output_file, index=False)\n",
    "#         print(f\"‚úÖ Processed {output_file} (shape: {train_df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47da3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [\n",
    "#     ('7GHz', './7GHz_Tx_1_train_data_with_coords.csv'),\n",
    "#     ('7GHz', './7GHz_Tx_2_train_data_with_coords.csv'),\n",
    "#     ('7GHz', './7GHz_Tx_8_train_data_with_coords.csv'),\n",
    "#     ('7GHz', './7GHz_Tx_18_train_data_with_coords.csv'),\n",
    "#     ('7GHz', './7GHz_Tx_19_train_data_with_coords.csv'),\n",
    "\n",
    "#     ('28GHz', './28GHz_Tx_1_train_data_with_coords.csv'),\n",
    "#     ('28GHz', './28GHz_Tx_2_train_data_with_coords.csv'),\n",
    "#     ('28GHz', './28GHz_Tx_8_train_data_with_coords.csv'),\n",
    "#     ('28GHz', './28GHz_Tx_18_train_data_with_coords.csv'),\n",
    "#     ('28GHz', './28GHz_Tx_19_train_data_with_coords.csv'),\n",
    "\n",
    "#     ('800MHz', './800MHz_Tx_1_train_data_with_coords.csv'),\n",
    "#     ('800MHz', './800MHz_Tx_2_train_data_with_coords.csv'),\n",
    "#     ('800MHz', './800MHz_Tx_8_train_data_with_coords.csv'),\n",
    "#     ('800MHz', './800MHz_Tx_18_train_data_with_coords.csv'),\n",
    "#     ('800MHz', './800MHz_Tx_19_train_data_with_coords.csv'),\n",
    "# ]\n",
    "\n",
    "# # Helper: Convert frequency string to numeric Hz\n",
    "# def freq_to_hz(freq_str):\n",
    "#     if \"GHz\" in freq_str:\n",
    "#         return float(freq_str.replace(\"GHz\", \"\")) * 1e9\n",
    "#     elif \"MHz\" in freq_str:\n",
    "#         return float(freq_str.replace(\"MHz\", \"\")) * 1e6\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unrecognized frequency format: {freq_str}\")\n",
    "\n",
    "# # Loop through all train datasets\n",
    "# for freq, path in datasets:\n",
    "#     try:\n",
    "#         df = pd.read_csv(path)\n",
    "#         center_freq = freq_to_hz(freq)\n",
    "#         df[\"CenterFreq_Hz\"] = center_freq\n",
    "\n",
    "#         # Save updated file (overwrite or create new one)\n",
    "#         output_path = path.replace(\"_with_coords\", \"_with_coords_freq\")\n",
    "#         df.to_csv(output_path, index=False)\n",
    "\n",
    "#         print(f\"‚úÖ Added CenterFreq_Hz={center_freq:.0f} to {output_path} (shape={df.shape})\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Error processing {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51414550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./800MHz_Tx_1_train_data_with_coords_freq.csv ‚Üí subset shape: (8000, 26)\n",
      "Combined subset shape: (400, 26)\n"
     ]
    }
   ],
   "source": [
    "def load_and_subset(data_path, index_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    idx = pd.read_csv(index_path, header=None).squeeze()  # assumes one column of indices\n",
    "    df_subset = df.loc[idx].reset_index(drop=True)\n",
    "    return df_subset\n",
    "\n",
    "# ---- Example mapping: each data file with its index file ----\n",
    "files = [\n",
    "#     ('./7GHz_Tx_1_train_data_with_coords_freq.csv', './7GHz_Tx_1_random_subset'),\n",
    "#     ('./7GHz_Tx_2_train_data_with_coords_freq.csv', './7GHz_Tx_2_random_subset'),\n",
    "#     ('./7GHz_Tx_8_train_data_with_coords_freq.csv', './7GHz_Tx_8_random_subset'),\n",
    "#     ('./7GHz_Tx_18_train_data_with_coords_freq.csv', './7GHz_Tx_18_random_subset'),\n",
    "#     ('./7GHz_Tx_19_train_data_with_coords_freq.csv', './7GHz_Tx_19_random_subset'),\n",
    "\n",
    "#     ('./28GHz_Tx_1_train_data_with_coords_freq.csv', './28GHz_Tx_1_random_subset'),\n",
    "#     ('./28GHz_Tx_2_train_data_with_coords_freq.csv', './28GHz_Tx_2_random_subset'),\n",
    "#     ('./28GHz_Tx_8_train_data_with_coords_freq.csv', './28GHz_Tx_8_random_subset'),\n",
    "#     ('./28GHz_Tx_18_train_data_with_coords_freq.csv', './28GHz_Tx_18_random_subset'),\n",
    "#     ('./28GHz_Tx_19_train_data_with_coords_freq.csv', './28GHz_Tx_19_random_subset'),\n",
    "\n",
    "    ('./800MHz_Tx_1_train_data_with_coords_freq.csv', './800MHz_Tx_1_random_subset'),\n",
    "#     ('./800MHz_Tx_2_train_data_with_coords_freq.csv', './800MHz_Tx_2_random_subset'),\n",
    "#     ('./800MHz_Tx_8_train_data_with_coords_freq.csv', './800MHz_Tx_8_random_subset'),\n",
    "#     ('./800MHz_Tx_18_train_data_with_coords_freq.csv', './800MHz_Tx_18_random_subset'),\n",
    "#     ('./800MHz_Tx_19_train_data_with_coords_freq.csv', './800MHz_Tx_19_random_subset'),\n",
    "]\n",
    "\n",
    "# ---- Loop through all and create subsets ----\n",
    "subset_dfs = []\n",
    "for data_file, index_file in files:\n",
    "    df_subset = load_and_subset(data_file, index_file)\n",
    "    subset_dfs.append(df_subset)\n",
    "    print(f\"Loaded {data_file} ‚Üí subset shape: {df_subset.shape}\")\n",
    "\n",
    "# # ---- Access individually if needed ----\n",
    "# df1_subset, df2_subset, df3_subset, df4_subset, df5_subset, \\\n",
    "# df6_subset, df7_subset, df8_subset, df9_subset, df10_subset, \\\n",
    "# df11_subset, df12_subset, df13_subset, df14_subset, df15_subset = subset_dfs\n",
    "\n",
    "# ---- (Optional) Combine all subsets into one big DataFrame ----\n",
    "df = pd.concat(subset_dfs, ignore_index=True)\n",
    "# df = df.sample(frac = 0.5)\n",
    "df = df.sample(frac = 0.05)\n",
    "# df = df.sample(frac = 0.0125)\n",
    "\n",
    "print(\"Combined subset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb1c5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['distance_m', 'fspl_dB', 'umi_pathloss_dB', 'tx_height_m', 'f3_polygon_count', 'los_intersection_count', 'dist_to_first_intersection_m', 'dist_to_last_intersection_m', 'is_los', 'fresnel3_obstr_poly_area', 'los_obstr_poly_area', 'f1_polygon_count', 'fresnel1_obstr_poly_area', 'tx_sphere_poly_count', 'tx_sphere_obstr_poly_area', 'avg_tx_clutter_height', 'rx_sphere_poly_count', 'rx_sphere_obstr_poly_area', 'avg_rx_clutter_height', 'pointnet_prediction', 'Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat', 'measured_pathloss_dB', 'CenterFreq_Hz']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09f830fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./7GHz_Tx_5_test_data_with_coords_freq.csv')\n",
    "df2 = pd.read_csv('./7GHz_Tx_9_test_data_with_coords_freq.csv')\n",
    "df3 = pd.read_csv('./7GHz_Tx_12_test_data_with_coords_freq.csv')\n",
    "df4 = pd.read_csv('./7GHz_Tx_14_test_data_with_coords_freq.csv')\n",
    "df5 = pd.read_csv('./7GHz_Tx_20_test_data_with_coords_freq.csv')\n",
    "\n",
    "df6 = pd.read_csv('./28GHz_Tx_5_test_data_with_coords_freq.csv')\n",
    "df7 = pd.read_csv('./28GHz_Tx_9_test_data_with_coords_freq.csv')\n",
    "df8 = pd.read_csv('./28GHz_Tx_12_test_data_with_coords_freq.csv')\n",
    "df9 = pd.read_csv('./28GHz_Tx_14_test_data_with_coords_freq.csv')\n",
    "df10 = pd.read_csv('./28GHz_Tx_20_test_data_with_coords_freq.csv')\n",
    "\n",
    "\n",
    "df11 = pd.read_csv('./800MHz_Tx_5_test_data_with_coords_freq.csv')\n",
    "df12 = pd.read_csv('./800MHz_Tx_9_test_data_with_coords_freq.csv')\n",
    "df13 = pd.read_csv('./800MHz_Tx_12_test_data_with_coords_freq.csv')\n",
    "df14 = pd.read_csv('./800MHz_Tx_14_test_data_with_coords_freq.csv')\n",
    "df15 = pd.read_csv('./800MHz_Tx_20_test_data_with_coords_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "974bca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df13], ignore_index=True)   #800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "653ecaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = df2[['distance_m', 'fspl_dB', 'umi_pathloss_dB', 'tx_height_m',\n",
    "#               'f3_polygon_count', 'los_intersection_count', 'dist_to_first_intersection_m', \n",
    "#               'dist_to_last_intersection_m', 'is_los', 'fresnel3_obstr_poly_area', 'los_obstr_poly_area',\n",
    "#               'f1_polygon_count', 'fresnel1_obstr_poly_area', 'tx_sphere_poly_count', 'tx_sphere_obstr_poly_area', \n",
    "#               'avg_tx_clutter_height', 'rx_sphere_poly_count', 'rx_sphere_obstr_poly_area', 'avg_rx_clutter_height',\n",
    "#              'Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat','CenterFreq_Hz']]\n",
    "\n",
    "\n",
    "X_test = df2[['distance_m', 'fspl_dB', 'umi_pathloss_dB', 'tx_height_m',\n",
    "              'f3_polygon_count', 'los_intersection_count', 'dist_to_first_intersection_m', \n",
    "              'dist_to_last_intersection_m', 'is_los', 'fresnel3_obstr_poly_area', 'los_obstr_poly_area',\n",
    "              'f1_polygon_count', 'fresnel1_obstr_poly_area', 'tx_sphere_poly_count', 'tx_sphere_obstr_poly_area', \n",
    "              'avg_tx_clutter_height', 'rx_sphere_poly_count', 'rx_sphere_obstr_poly_area', 'avg_rx_clutter_height',\n",
    "             'Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat']]\n",
    "\n",
    "# X_test = df2[['distance_m', 'umi_pathloss_dB', 'tx_height_m','Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat','CenterFreq_Hz']]\n",
    "y_test = df2[['measured_pathloss_dB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88769d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the input features\n",
    "# X_train = df[['distance_m', 'fspl_dB', 'umi_pathloss_dB', 'tx_height_m',\n",
    "#               'f3_polygon_count', 'los_intersection_count', 'dist_to_first_intersection_m', \n",
    "#               'dist_to_last_intersection_m', 'is_los', 'fresnel3_obstr_poly_area', 'los_obstr_poly_area',\n",
    "#               'f1_polygon_count', 'fresnel1_obstr_poly_area', 'tx_sphere_poly_count', 'tx_sphere_obstr_poly_area', \n",
    "#               'avg_tx_clutter_height', 'rx_sphere_poly_count', 'rx_sphere_obstr_poly_area', 'avg_rx_clutter_height', \n",
    "#               'pointnet_prediction']]\n",
    "# X_train = df[['distance_m', 'fspl_dB', 'umi_pathloss_dB', 'tx_height_m',\n",
    "#               'f3_polygon_count', 'los_intersection_count', 'dist_to_first_intersection_m', \n",
    "#               'dist_to_last_intersection_m', 'is_los', 'fresnel3_obstr_poly_area', 'los_obstr_poly_area',\n",
    "#               'f1_polygon_count', 'fresnel1_obstr_poly_area', 'tx_sphere_poly_count', 'tx_sphere_obstr_poly_area', \n",
    "#               'avg_tx_clutter_height', 'rx_sphere_poly_count', 'rx_sphere_obstr_poly_area', 'avg_rx_clutter_height',\n",
    "#               'Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat','CenterFreq_Hz']]\n",
    "\n",
    "\n",
    "X_train = df[['distance_m', 'fspl_dB', 'umi_pathloss_dB', 'tx_height_m',\n",
    "              'f3_polygon_count', 'los_intersection_count', 'dist_to_first_intersection_m', \n",
    "              'dist_to_last_intersection_m', 'is_los', 'fresnel3_obstr_poly_area', 'los_obstr_poly_area',\n",
    "              'f1_polygon_count', 'fresnel1_obstr_poly_area', 'tx_sphere_poly_count', 'tx_sphere_obstr_poly_area', \n",
    "              'avg_tx_clutter_height', 'rx_sphere_poly_count', 'rx_sphere_obstr_poly_area', 'avg_rx_clutter_height',\n",
    "              'Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat']]\n",
    "\n",
    "# X_train = df[['distance_m', 'umi_pathloss_dB', 'tx_height_m','Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat','CenterFreq_Hz']]\n",
    "#set the target variable\n",
    "y_train = df[['measured_pathloss_dB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cbf33e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = df2[['distance_m', 'fspl_dB', 'umi_pathloss_dB', 'tx_height_m',\n",
    "              'f3_polygon_count', 'los_intersection_count', 'dist_to_first_intersection_m', \n",
    "              'dist_to_last_intersection_m', 'is_los', 'fresnel3_obstr_poly_area', 'los_obstr_poly_area',\n",
    "              'f1_polygon_count', 'fresnel1_obstr_poly_area', 'tx_sphere_poly_count', 'tx_sphere_obstr_poly_area', \n",
    "              'avg_tx_clutter_height', 'rx_sphere_poly_count', 'rx_sphere_obstr_poly_area', 'avg_rx_clutter_height',\n",
    "             'Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat']]\n",
    "\n",
    "# X_test = df2[['distance_m', 'umi_pathloss_dB', 'tx_height_m','Tx_Lon', 'Tx_Lat', 'Rx_Lon', 'Rx_Lat','CenterFreq_Hz']]\n",
    "y_valid = df2[['measured_pathloss_dB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39a17448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      measured_pathloss_dB\n",
      "2215             98.382195\n",
      "2582            124.151909\n",
      "1662            119.537445\n",
      "3027            131.411621\n",
      "4343            115.419434\n",
      "...                    ...\n",
      "6851            104.830780\n",
      "1972            101.205910\n",
      "6527            101.457291\n",
      "7918            118.991615\n",
      "3733            123.696854\n",
      "\n",
      "[400 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6381dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 01:51:18,325] A new study created in memory with name: no-name-8b89a9b5-61b9-4dff-b951-fda63a123352\n",
      "[I 2025-11-01 01:51:19,053] Trial 0 finished with value: 19.01787646729108 and parameters: {'iterations': 1080, 'learning_rate': 0.039025552497958506, 'depth': 8, 'l2_leaf_reg': 5.838311379533672, 'subsample': 0.5987443051665071, 'colsample_bylevel': 0.8576015449301211}. Best is trial 0 with value: 19.01787646729108.\n",
      "[I 2025-11-01 01:51:19,289] Trial 1 finished with value: 19.268915463588467 and parameters: {'iterations': 1107, 'learning_rate': 0.07539888384154877, 'depth': 8, 'l2_leaf_reg': 0.6649674415387996, 'subsample': 0.9114013653380599, 'colsample_bylevel': 0.9957582322466257}. Best is trial 0 with value: 19.01787646729108.\n",
      "[I 2025-11-01 01:51:19,946] Trial 2 finished with value: 19.291000270179232 and parameters: {'iterations': 739, 'learning_rate': 0.004860022194172711, 'depth': 6, 'l2_leaf_reg': 5.053585446435495, 'subsample': 0.8636131202707533, 'colsample_bylevel': 0.9391601699608991}. Best is trial 0 with value: 19.01787646729108.\n",
      "[I 2025-11-01 01:51:20,067] Trial 3 finished with value: 19.086971032562804 and parameters: {'iterations': 1277, 'learning_rate': 0.06091430359590333, 'depth': 5, 'l2_leaf_reg': 2.4368578529912828, 'subsample': 0.9435794330059664, 'colsample_bylevel': 0.5678646137507966}. Best is trial 0 with value: 19.01787646729108.\n",
      "[I 2025-11-01 01:51:20,157] Trial 4 finished with value: 18.762352092351318 and parameters: {'iterations': 1392, 'learning_rate': 0.08702698542039354, 'depth': 1, 'l2_leaf_reg': 2.4926153671220344, 'subsample': 0.6822936815064335, 'colsample_bylevel': 0.9824206452758559}. Best is trial 4 with value: 18.762352092351318.\n",
      "[I 2025-11-01 01:51:20,322] Trial 5 finished with value: 18.892730223752014 and parameters: {'iterations': 1284, 'learning_rate': 0.06816487795016886, 'depth': 7, 'l2_leaf_reg': 0.9992028643425599, 'subsample': 0.6090269673399852, 'colsample_bylevel': 0.8296833714171571}. Best is trial 4 with value: 18.762352092351318.\n",
      "[I 2025-11-01 01:51:21,074] Trial 6 finished with value: 19.205436171013044 and parameters: {'iterations': 1295, 'learning_rate': 0.08172839643469847, 'depth': 8, 'l2_leaf_reg': 8.52634363522805, 'subsample': 0.6399795364335782, 'colsample_bylevel': 0.5983823820003644}. Best is trial 4 with value: 18.762352092351318.\n",
      "[I 2025-11-01 01:51:22,214] Trial 7 finished with value: 19.23475705845403 and parameters: {'iterations': 364, 'learning_rate': 0.012457089637302281, 'depth': 8, 'l2_leaf_reg': 0.35255410202266546, 'subsample': 0.6154641887969431, 'colsample_bylevel': 0.8153384718616595}. Best is trial 4 with value: 18.762352092351318.\n",
      "[I 2025-11-01 01:51:23,112] Trial 8 finished with value: 18.91958767366545 and parameters: {'iterations': 294, 'learning_rate': 0.056024071211831554, 'depth': 10, 'l2_leaf_reg': 8.625564636809807, 'subsample': 0.9355591135720139, 'colsample_bylevel': 0.6794898824109892}. Best is trial 4 with value: 18.762352092351318.\n",
      "[I 2025-11-01 01:51:23,208] Trial 9 finished with value: 19.165394696256616 and parameters: {'iterations': 1138, 'learning_rate': 0.048959669091860634, 'depth': 2, 'l2_leaf_reg': 0.8041148518555686, 'subsample': 0.9065839856765712, 'colsample_bylevel': 0.8860477550565504}. Best is trial 4 with value: 18.762352092351318.\n",
      "[I 2025-11-01 01:51:23,344] Trial 10 finished with value: 18.7229751138997 and parameters: {'iterations': 680, 'learning_rate': 0.0966497211248935, 'depth': 1, 'l2_leaf_reg': 3.35384485926704, 'subsample': 0.7385229503077483, 'colsample_bylevel': 0.7089528677873069}. Best is trial 10 with value: 18.7229751138997.\n",
      "[I 2025-11-01 01:51:23,424] Trial 11 finished with value: 18.770341147388784 and parameters: {'iterations': 698, 'learning_rate': 0.09790350271763518, 'depth': 1, 'l2_leaf_reg': 2.9988491223831932, 'subsample': 0.7415161013353906, 'colsample_bylevel': 0.7190039771905352}. Best is trial 10 with value: 18.7229751138997.\n",
      "[I 2025-11-01 01:51:23,533] Trial 12 finished with value: 18.922297024411638 and parameters: {'iterations': 536, 'learning_rate': 0.09357410004508342, 'depth': 3, 'l2_leaf_reg': 3.8167551725505047, 'subsample': 0.7394465969875132, 'colsample_bylevel': 0.6603467264511671}. Best is trial 10 with value: 18.7229751138997.\n",
      "[I 2025-11-01 01:51:23,665] Trial 13 finished with value: 19.13442946770767 and parameters: {'iterations': 1495, 'learning_rate': 0.08657338369081864, 'depth': 4, 'l2_leaf_reg': 6.663342100934541, 'subsample': 0.5244316659334466, 'colsample_bylevel': 0.7610082811413339}. Best is trial 10 with value: 18.7229751138997.\n",
      "[I 2025-11-01 01:51:23,769] Trial 14 finished with value: 18.87307388797082 and parameters: {'iterations': 903, 'learning_rate': 0.0995898550253001, 'depth': 1, 'l2_leaf_reg': 3.8910639532745726, 'subsample': 0.8020199311956966, 'colsample_bylevel': 0.7574568342818659}. Best is trial 10 with value: 18.7229751138997.\n",
      "[I 2025-11-01 01:51:23,952] Trial 15 finished with value: 18.98044656891885 and parameters: {'iterations': 543, 'learning_rate': 0.027431241882580528, 'depth': 3, 'l2_leaf_reg': 2.0239560981847795, 'subsample': 0.6999537276284183, 'colsample_bylevel': 0.5032965735827247}. Best is trial 10 with value: 18.7229751138997.\n",
      "[I 2025-11-01 01:51:24,045] Trial 16 finished with value: 19.16607870349369 and parameters: {'iterations': 905, 'learning_rate': 0.07291001543779116, 'depth': 2, 'l2_leaf_reg': 3.9997696870055344, 'subsample': 0.8179302567567392, 'colsample_bylevel': 0.9993473492780841}. Best is trial 10 with value: 18.7229751138997.\n",
      "[I 2025-11-01 01:51:24,145] Trial 17 finished with value: 18.684707983151455 and parameters: {'iterations': 1491, 'learning_rate': 0.0789528198721598, 'depth': 1, 'l2_leaf_reg': 7.358744425531777, 'subsample': 0.6576333205766673, 'colsample_bylevel': 0.9143925506303029}. Best is trial 17 with value: 18.684707983151455.\n",
      "[I 2025-11-01 01:51:24,251] Trial 18 finished with value: 19.01810125590366 and parameters: {'iterations': 161, 'learning_rate': 0.04428832281144368, 'depth': 3, 'l2_leaf_reg': 9.96343243607213, 'subsample': 0.5196302096353588, 'colsample_bylevel': 0.9083879161085463}. Best is trial 17 with value: 18.684707983151455.\n",
      "[I 2025-11-01 01:51:24,354] Trial 19 finished with value: 19.386431305099 and parameters: {'iterations': 557, 'learning_rate': 0.06466830801755495, 'depth': 5, 'l2_leaf_reg': 6.6062116928948456, 'subsample': 0.7947717747990758, 'colsample_bylevel': 0.8005282092009336}. Best is trial 17 with value: 18.684707983151455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE: 18.684707983151455\n",
      "Best Parameters: {'iterations': 1491, 'learning_rate': 0.0789528198721598, 'depth': 1, 'l2_leaf_reg': 7.358744425531777, 'subsample': 0.6576333205766673, 'colsample_bylevel': 0.9143925506303029}\n",
      "üéØ Optuna tuning time: 0.10 minutes\n",
      "0:\tlearn: 8.9094930\ttest: 17.7458389\tbest: 17.7458389 (0)\ttotal: 277us\tremaining: 414ms\n",
      "100:\tlearn: 6.5255338\ttest: 15.3114973\tbest: 15.3114973 (100)\ttotal: 22.2ms\tremaining: 305ms\n",
      "200:\tlearn: 6.2481295\ttest: 15.1191104\tbest: 15.1191104 (200)\ttotal: 42.6ms\tremaining: 273ms\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 15.0921928\n",
      "bestIteration = 211\n",
      "\n",
      "Shrink model to first 212 iterations.\n",
      "‚è±Ô∏è Final model training time: 0.07 seconds\n",
      "‚úÖ Model saved as best_catboost_model.cbm\n",
      "‚úÖ Best parameters saved to ./best_optuna_params.txt\n",
      "üìÑ Training summary saved to training_runtime_summary.txt\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "import time\n",
    "\n",
    "# Step 1: Data Splitting (Training and Validation Only)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Identify and Mark Categorical Features\n",
    "cat_features = [X_train.columns.get_loc(col) for col in X_train.select_dtypes(include=['object', 'category']).columns]\n",
    "\n",
    "# Step 3: Optuna Hyperparameter Tuning\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0)\n",
    "    }\n",
    "\n",
    "    # Initialize CatBoost model with sampled parameters\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"MAE\",\n",
    "        cat_features=cat_features,\n",
    "        verbose=0,  # Disable training output for optimization\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50)\n",
    "\n",
    "    # Predict and evaluate performance\n",
    "    y_pred = model.predict(X_valid)\n",
    "    return np.sqrt(mean_squared_error(y_valid, y_pred)) \n",
    "\n",
    "start_optuna = time.time()\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "end_optuna = time.time()\n",
    "optuna_time = end_optuna - start_optuna\n",
    "\n",
    "# Step 5: Output the Best Results\n",
    "best_params = study.best_params\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(f\"üéØ Optuna tuning time: {optuna_time/60:.2f} minutes\")\n",
    "\n",
    "# Step 6: Refit Final Model Using Best Parameters\n",
    "model = CatBoostRegressor(\n",
    "    loss_function=\"MAE\",\n",
    "    cat_features=cat_features,\n",
    "    verbose=100,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "# Measure model training time\n",
    "start_train = time.time()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_valid, y_valid),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "end_train = time.time()\n",
    "train_time = end_train - start_train\n",
    "\n",
    "print(f\"‚è±Ô∏è Final model training time: {train_time:.2f} seconds\")\n",
    "\n",
    "# Step 7: Save model and parameters\n",
    "model.save_model(\"best_catboost_model.cbm\")\n",
    "print(\"‚úÖ Model saved as best_catboost_model.cbm\")\n",
    "\n",
    "best_params_file = \"./best_optuna_params.txt\"\n",
    "with open(best_params_file, \"w\") as f:\n",
    "    f.write(str(best_params))\n",
    "print(\"‚úÖ Best parameters saved to\", best_params_file)\n",
    "\n",
    "# Step 8: Save runtime results\n",
    "with open(\"training_runtime_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"Best RMSE: {study.best_value:.4f}\\n\")\n",
    "    f.write(f\"Optuna tuning time: {optuna_time/60:.2f} minutes\\n\")\n",
    "    f.write(f\"Final model training time: {train_time:.2f} seconds\\n\")\n",
    "print(\"üìÑ Training summary saved to training_runtime_summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1f157c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs = [\"7GHz\", \"28GHz\", \"800MHz\"]\n",
    "# tx_ids = [5, 9, 12, 14, 20]\n",
    "\n",
    "# for freq in freqs:\n",
    "#     for txid in tx_ids:\n",
    "#         meta_file = f\"./{freq}_Tx_{txid}.csv\"\n",
    "#         train_file = f\"./{freq}_Tx_{txid}_test_data.csv\"\n",
    "#         output_file = f\"./{freq}_Tx_{txid}_test_data_with_coords.csv\"\n",
    "\n",
    "#         if not os.path.exists(meta_file) or not os.path.exists(train_file):\n",
    "#             print(f\"Skipping {freq}_Tx_{txid} ‚Äî missing file(s).\")\n",
    "#             continue\n",
    "\n",
    "#         # --- Read metadata (Tx and Rx coordinates) ---\n",
    "#         meta_df = pd.read_csv(meta_file)\n",
    "#         tx_lon = meta_df.iloc[:, 3].values  # 4th column (Tx_Lon)\n",
    "#         tx_lat = meta_df.iloc[:, 4].values  # 5th column (Tx_Lat)\n",
    "#         rx_lon = meta_df[\"Rx_lon\"].values\n",
    "#         rx_lat = meta_df[\"Rx_lat\"].values\n",
    "\n",
    "#         # --- Read train data ---\n",
    "#         train_df = pd.read_csv(train_file)\n",
    "\n",
    "#         # --- Prepare coordinate DataFrame ---\n",
    "#         coords_df = pd.DataFrame({\n",
    "#             \"Tx_Lon\": tx_lon,\n",
    "#             \"Tx_Lat\": tx_lat,\n",
    "#             \"Rx_Lon\": rx_lon,\n",
    "#             \"Rx_Lat\": rx_lat\n",
    "#         })\n",
    "\n",
    "#         # --- Insert before the last column ---\n",
    "#         insert_pos = len(train_df.columns) - 1\n",
    "#         for col_name in [\"Tx_Lon\", \"Tx_Lat\", \"Rx_Lon\", \"Rx_Lat\"]:\n",
    "#             train_df.insert(insert_pos, col_name, coords_df[col_name])\n",
    "#             insert_pos += 1  # shift forward for next column\n",
    "\n",
    "#         # --- Save output ---\n",
    "#         train_df.to_csv(output_file, index=False)\n",
    "#         print(f\"‚úÖ Processed {output_file} (shape: {train_df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0480032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [\n",
    "#     ('7GHz', './7GHz_Tx_5_test_data_with_coords.csv'),\n",
    "#     ('7GHz', './7GHz_Tx_9_test_data_with_coords.csv'),\n",
    "#     ('7GHz', './7GHz_Tx_12_test_data_with_coords.csv'),\n",
    "#     ('7GHz', './7GHz_Tx_14_test_data_with_coords.csv'),\n",
    "#     ('7GHz', './7GHz_Tx_20_test_data_with_coords.csv'),\n",
    "\n",
    "#     ('28GHz', './28GHz_Tx_5_test_data_with_coords.csv'),\n",
    "#     ('28GHz', './28GHz_Tx_9_test_data_with_coords.csv'),\n",
    "#     ('28GHz', './28GHz_Tx_12_test_data_with_coords.csv'),\n",
    "#     ('28GHz', './28GHz_Tx_14_test_data_with_coords.csv'),\n",
    "#     ('28GHz', './28GHz_Tx_20_test_data_with_coords.csv'),\n",
    "\n",
    "#     ('800MHz', './800MHz_Tx_5_test_data_with_coords.csv'),\n",
    "#     ('800MHz', './800MHz_Tx_9_test_data_with_coords.csv'),\n",
    "#     ('800MHz', './800MHz_Tx_12_test_data_with_coords.csv'),\n",
    "#     ('800MHz', './800MHz_Tx_14_test_data_with_coords.csv'),\n",
    "#     ('800MHz', './800MHz_Tx_20_test_data_with_coords.csv'),\n",
    "# ]\n",
    "\n",
    "# # Helper: Convert frequency string to numeric Hz\n",
    "# def freq_to_hz(freq_str):\n",
    "#     if \"GHz\" in freq_str:\n",
    "#         return float(freq_str.replace(\"GHz\", \"\")) * 1e9\n",
    "#     elif \"MHz\" in freq_str:\n",
    "#         return float(freq_str.replace(\"MHz\", \"\")) * 1e6\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unrecognized frequency format: {freq_str}\")\n",
    "\n",
    "# # Loop through all test datasets\n",
    "# for freq, path in datasets:\n",
    "#     try:\n",
    "#         df = pd.read_csv(path)\n",
    "#         center_freq = freq_to_hz(freq)\n",
    "#         df[\"CenterFreq_Hz\"] = center_freq\n",
    "\n",
    "#         # Save updated file (overwrite or create new one)\n",
    "#         output_path = path.replace(\"_with_coords\", \"_with_coords_freq\")\n",
    "#         df.to_csv(output_path, index=False)\n",
    "\n",
    "#         print(f\"‚úÖ Added CenterFreq_Hz={center_freq:.0f} to {output_path} (shape={df.shape})\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Error processing {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f9714bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "862f0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.concat([df11, df12, df13, df14, df15], ignore_index=True)   #800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32c108bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)       #7G\n",
    "# df2 = pd.concat([df5], ignore_index=True)       #7G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e8ebd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.concat([df6, df7, df8, df9, df10], ignore_index=True)       #28G\n",
    "# df2 = pd.concat([df10], ignore_index=True)       #28G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc6197d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.concat([df1, df2, df3, df4, df5, df11, df12, df13, df14, df15], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd73bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54e42462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load the saved model\n",
    "model = CatBoostRegressor()\n",
    "model.load_model(\"best_catboost_model.cbm\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d33fad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 18.6847\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1623e705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (24022, 14)\n",
      "Predictions length: 24022\n",
      "‚úÖ Predictions added and saved to Result_Task1_800MHz_Tx_12.csv (shape=(24022, 14))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original file\n",
    "# file_path = \"Result_Task1_7GHz_Tx_20.csv\"\n",
    "# file_path = \"Result_Task1_28GHz_Tx_20.csv\"\n",
    "file_path = \"Result_Task1_800MHz_Tx_12.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure y_pred has the same length as df\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Predictions length:\", len(y_pred))\n",
    "\n",
    "if len(y_pred) != len(df):\n",
    "    raise ValueError(\"‚ùå Length mismatch: y_pred and CSV rows must match!\")\n",
    "\n",
    "# Add predictions as the last column\n",
    "df[\"Predicted PL\"] = y_pred\n",
    "\n",
    "# Save updated file (overwrite or create new one)\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Predictions added and saved to {file_path} (shape={df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246316fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
